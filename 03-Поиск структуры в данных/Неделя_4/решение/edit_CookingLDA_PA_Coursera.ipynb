{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment: \n",
    "## Готовим LDA по рецептам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как вы уже знаете, в тематическом моделировании делается предположение о том, что для определения тематики порядок слов в документе не важен; об этом гласит гипотеза «мешка слов». Сегодня мы будем работать с несколько нестандартной для тематического моделирования коллекцией, которую можно назвать «мешком ингредиентов», потому что на состоит из рецептов блюд разных кухонь. Тематические модели ищут слова, которые часто вместе встречаются в документах, и составляют из них темы. Мы попробуем применить эту идею к рецептам и найти кулинарные «темы». Эта коллекция хороша тем, что не требует предобработки. Кроме того, эта задача достаточно наглядно иллюстрирует принцип работы тематических моделей.\n",
    "\n",
    "Для выполнения заданий, помимо часто используемых в курсе библиотек, потребуются модули *json* и *gensim*. Первый входит в дистрибутив Anaconda, второй можно поставить командой \n",
    "\n",
    "*pip install gensim*\n",
    "\n",
    "Построение модели занимает некоторое время. На ноутбуке с процессором Intel Core i7 и тактовой частотой 2400 МГц на построение одной модели уходит менее 10 минут."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коллекция дана в json-формате: для каждого рецепта известны его id, кухня (cuisine) и список ингредиентов, в него входящих. Загрузить данные можно с помощью модуля json (он входит в дистрибутив Anaconda):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"recipes.json\") as f:\n",
    "    recipes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 10259, 'cuisine': 'greek', 'ingredients': ['romaine lettuce', 'black olives', 'grape tomatoes', 'garlic', 'pepper', 'purple onion', 'seasoning', 'garbanzo beans', 'feta cheese crumbles']}\n"
     ]
    }
   ],
   "source": [
    "print(recipes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(recipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Составление корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша коллекция небольшая, и целиком помещается в оперативную память. Gensim может работать с такими данными и не требует их сохранения на диск в специальном формате. Для этого коллекция должна быть представлена в виде списка списков, каждый внутренний список соответствует отдельному документу и состоит из его слов. Пример коллекции из двух документов: \n",
    "\n",
    "[[\"hello\", \"world\"], [\"programming\", \"in\", \"python\"]]\n",
    "\n",
    "Преобразуем наши данные в такой формат, а затем создадим объекты corpus и dictionary, с которыми будет работать модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [recipe[\"ingredients\"] for recipe in recipes]\n",
    "dictionary = corpora.Dictionary(texts)   # составляем словарь\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]  # составляем корпус документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['romaine lettuce', 'black olives', 'grape tomatoes', 'garlic', 'pepper', 'purple onion', 'seasoning', 'garbanzo beans', 'feta cheese crumbles']\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['green chile', 'jalapeno chilies', 'onions', 'ground black pepper', 'salt', 'chopped cilantro fresh', 'green bell pepper', 'garlic', 'white sugar', 'roma tomatoes', 'celery', 'dried oregano']\n",
      "[(3, 1), (11, 1), (15, 1), (44, 1), (57, 1), (69, 1), (81, 1), (157, 1), (196, 1), (209, 1), (231, 1), (733, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(texts[-1])\n",
    "print(corpus[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_1=0\n",
    "for i in range(len(corpus)):\n",
    "    for j in range(len(corpus[i])):\n",
    "        if corpus[i][j][0]>max_1:\n",
    "            max_1=corpus[i][j][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_2=0\n",
    "for i in range(len(corpus2)):\n",
    "    for j in range(len(corpus2[i])):\n",
    "        if corpus2[i][j][0]>max_2:\n",
    "            max_2=corpus2[i][j][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6701"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6713"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(corpus2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39774"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39774"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on dict object:\n",
      "\n",
      "class dict(object)\n",
      " |  dict() -> new empty dictionary\n",
      " |  dict(mapping) -> new dictionary initialized from a mapping object's\n",
      " |      (key, value) pairs\n",
      " |  dict(iterable) -> new dictionary initialized as if via:\n",
      " |      d = {}\n",
      " |      for k, v in iterable:\n",
      " |          d[k] = v\n",
      " |  dict(**kwargs) -> new dictionary initialized with the name=value pairs\n",
      " |      in the keyword argument list.  For example:  dict(one=1, two=2)\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      True if the dictionary has the specified key, else False.\n",
      " |  \n",
      " |  __delitem__(self, key, /)\n",
      " |      Delete self[key].\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __getitem__(...)\n",
      " |      x.__getitem__(y) <==> x[y]\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __init__(self, /, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setitem__(self, key, value, /)\n",
      " |      Set self[key] to value.\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      D.__sizeof__() -> size of D in memory, in bytes\n",
      " |  \n",
      " |  clear(...)\n",
      " |      D.clear() -> None.  Remove all items from D.\n",
      " |  \n",
      " |  copy(...)\n",
      " |      D.copy() -> a shallow copy of D\n",
      " |  \n",
      " |  get(self, key, default=None, /)\n",
      " |      Return the value for key if key is in the dictionary, else default.\n",
      " |  \n",
      " |  items(...)\n",
      " |      D.items() -> a set-like object providing a view on D's items\n",
      " |  \n",
      " |  keys(...)\n",
      " |      D.keys() -> a set-like object providing a view on D's keys\n",
      " |  \n",
      " |  pop(...)\n",
      " |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      " |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
      " |  \n",
      " |  popitem(...)\n",
      " |      D.popitem() -> (k, v), remove and return some (key, value) pair as a\n",
      " |      2-tuple; but raise KeyError if D is empty.\n",
      " |  \n",
      " |  setdefault(self, key, default=None, /)\n",
      " |      Insert key with a value of default if key is not in the dictionary.\n",
      " |      \n",
      " |      Return the value for key if key is in the dictionary, else default.\n",
      " |  \n",
      " |  update(...)\n",
      " |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      " |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      " |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      " |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      " |  \n",
      " |  values(...)\n",
      " |      D.values() -> an object providing a view on D's values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  fromkeys(iterable, value=None, /) from builtins.type\n",
      " |      Create a new dictionary with keys from iterable and values set to value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict2=dict()\n",
    "for key in dictionary.token2id:\n",
    "    dict2[dictionary.token2id[key]]=key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'black olives',\n",
       " 1: 'feta cheese crumbles',\n",
       " 2: 'garbanzo beans',\n",
       " 3: 'garlic',\n",
       " 4: 'grape tomatoes',\n",
       " 5: 'pepper',\n",
       " 6: 'purple onion',\n",
       " 7: 'romaine lettuce',\n",
       " 8: 'seasoning',\n",
       " 9: 'eggs',\n",
       " 10: 'green tomatoes',\n",
       " 11: 'ground black pepper',\n",
       " 12: 'ground pepper',\n",
       " 13: 'milk',\n",
       " 14: 'plain flour',\n",
       " 15: 'salt',\n",
       " 16: 'thyme',\n",
       " 17: 'tomatoes',\n",
       " 18: 'vegetable oil',\n",
       " 19: 'yellow corn meal',\n",
       " 20: 'butter',\n",
       " 21: 'chicken livers',\n",
       " 22: 'cooking oil',\n",
       " 23: 'garlic powder',\n",
       " 24: 'green chilies',\n",
       " 25: 'grilled chicken breasts',\n",
       " 26: 'mayonaise',\n",
       " 27: 'soy sauce',\n",
       " 28: 'yellow onion',\n",
       " 29: 'water',\n",
       " 30: 'wheat',\n",
       " 31: 'bay leaf',\n",
       " 32: 'black pepper',\n",
       " 33: 'boneless chicken skinless thigh',\n",
       " 34: 'cayenne pepper',\n",
       " 35: 'chili powder',\n",
       " 36: 'cornflour',\n",
       " 37: 'double cream',\n",
       " 38: 'garam masala',\n",
       " 39: 'garlic paste',\n",
       " 40: 'ground cumin',\n",
       " 41: 'lemon juice',\n",
       " 42: 'natural yogurt',\n",
       " 43: 'oil',\n",
       " 44: 'onions',\n",
       " 45: 'passata',\n",
       " 46: 'shallots',\n",
       " 47: 'baking powder',\n",
       " 48: 'fresh ginger root',\n",
       " 49: 'ground cinnamon',\n",
       " 50: 'ground ginger',\n",
       " 51: 'powdered sugar',\n",
       " 52: 'sugar',\n",
       " 53: 'vanilla extract',\n",
       " 54: 'chopped cilantro',\n",
       " 55: 'chorizo sausage',\n",
       " 56: 'flat leaf parsley',\n",
       " 57: 'jalapeno chilies',\n",
       " 58: 'medium shrimp',\n",
       " 59: 'olive oil',\n",
       " 60: 'sea salt',\n",
       " 61: 'skirt steak',\n",
       " 62: 'white vinegar',\n",
       " 63: 'almond extract',\n",
       " 64: 'dried cranberries',\n",
       " 65: 'flour',\n",
       " 66: 'pistachio nuts',\n",
       " 67: 'white almond bark',\n",
       " 68: 'cheddar cheese',\n",
       " 69: 'chopped cilantro fresh',\n",
       " 70: 'corn tortillas',\n",
       " 71: 'fresh pineapple',\n",
       " 72: 'iceberg lettuce',\n",
       " 73: 'lime',\n",
       " 74: 'poblano peppers',\n",
       " 75: 'pork',\n",
       " 76: 'chopped tomatoes',\n",
       " 77: 'extra-virgin olive oil',\n",
       " 78: 'fresh basil',\n",
       " 79: 'kosher salt',\n",
       " 80: 'canola oil',\n",
       " 81: 'dried oregano',\n",
       " 82: 'mushrooms',\n",
       " 83: 'pimentos',\n",
       " 84: 'provolone cheese',\n",
       " 85: 'sausages',\n",
       " 86: 'sharp cheddar cheese',\n",
       " 87: 'sweet pepper',\n",
       " 88: 'swiss cheese',\n",
       " 89: 'Shaoxing wine',\n",
       " 90: 'corn starch',\n",
       " 91: 'crushed red pepper flakes',\n",
       " 92: 'dry mustard',\n",
       " 93: 'fresh ginger',\n",
       " 94: 'green beans',\n",
       " 95: 'ground turkey',\n",
       " 96: 'low sodium soy sauce',\n",
       " 97: 'scallions',\n",
       " 98: 'sesame oil',\n",
       " 99: 'white pepper',\n",
       " 100: 'Italian parsley leaves',\n",
       " 101: 'chipotle chile',\n",
       " 102: 'fine sea salt',\n",
       " 103: 'fresh lemon juice',\n",
       " 104: 'garlic cloves',\n",
       " 105: 'hot red pepper flakes',\n",
       " 106: 'trout fillet',\n",
       " 107: 'walnuts',\n",
       " 108: 'avocado',\n",
       " 109: 'flank steak',\n",
       " 110: 'fresh cilantro',\n",
       " 111: 'ground coriander',\n",
       " 112: 'lime juice',\n",
       " 113: 'plum tomatoes',\n",
       " 114: 'all-purpose flour',\n",
       " 115: 'bacon slices',\n",
       " 116: 'chopped fresh chives',\n",
       " 117: 'cooking spray',\n",
       " 118: 'fat free less sodium chicken broth',\n",
       " 119: 'fat free milk',\n",
       " 120: 'fresh parmesan cheese',\n",
       " 121: 'gnocchi',\n",
       " 122: 'gruyere cheese',\n",
       " 123: 'naan',\n",
       " 124: 'red chili peppers',\n",
       " 125: 'red lentils',\n",
       " 126: 'spinach',\n",
       " 127: 'sweet potatoes',\n",
       " 128: 'tumeric',\n",
       " 129: 'vegetable stock',\n",
       " 130: 'confectioners sugar',\n",
       " 131: 'greek yogurt',\n",
       " 132: 'lemon curd',\n",
       " 133: 'raspberries',\n",
       " 134: 'broiler-fryer chicken',\n",
       " 135: 'italian seasoning',\n",
       " 136: 'zesty italian dressing',\n",
       " 137: 'asian fish sauce',\n",
       " 138: 'hot chili',\n",
       " 139: 'broccolini',\n",
       " 140: 'chicken breasts',\n",
       " 141: 'chicken broth',\n",
       " 142: 'cooked rice',\n",
       " 143: 'fresh lime juice',\n",
       " 144: 'garlic chili sauce',\n",
       " 145: 'red bell pepper',\n",
       " 146: 'sliced green onions',\n",
       " 147: 'yellow squash',\n",
       " 148: 'beansprouts',\n",
       " 149: 'chopped fresh mint',\n",
       " 150: 'creamy peanut butter',\n",
       " 151: 'hoisin sauce',\n",
       " 152: 'pork loin',\n",
       " 153: 'rice',\n",
       " 154: 'rice noodles',\n",
       " 155: 'roasted peanuts',\n",
       " 156: 'thai basil',\n",
       " 157: 'roma tomatoes',\n",
       " 158: 'baking potatoes',\n",
       " 159: 'low-fat mayonnaise',\n",
       " 160: 'spicy brown mustard',\n",
       " 161: 'arrowroot powder',\n",
       " 162: 'broccoli',\n",
       " 163: 'extra firm tofu',\n",
       " 164: 'orange bell pepper',\n",
       " 165: 'red curry paste',\n",
       " 166: 'red pepper',\n",
       " 167: 'sesame seeds',\n",
       " 168: 'yellow peppers',\n",
       " 169: 'capers',\n",
       " 170: 'lemon zest',\n",
       " 171: 'linguine',\n",
       " 172: 'marinara sauce',\n",
       " 173: 'olives',\n",
       " 174: 'cabbage',\n",
       " 175: 'chives',\n",
       " 176: 'dark soy sauce',\n",
       " 177: 'dried black mushrooms',\n",
       " 178: 'light soy sauce',\n",
       " 179: 'lo mein noodles',\n",
       " 180: 'oyster sauce',\n",
       " 181: 'peanuts',\n",
       " 182: 'chile pepper',\n",
       " 183: 'fresh tomatoes',\n",
       " 184: 'herbs',\n",
       " 185: 'mango',\n",
       " 186: 'paprika',\n",
       " 187: 'stock',\n",
       " 188: 'cooked chicken',\n",
       " 189: 'grated parmesan cheese',\n",
       " 190: 'heavy cream',\n",
       " 191: 'sherry',\n",
       " 192: 'sliced mushrooms',\n",
       " 193: 'spaghetti',\n",
       " 194: 'carrots',\n",
       " 195: 'egg roll wrappers',\n",
       " 196: 'green bell pepper',\n",
       " 197: 'ground pork',\n",
       " 198: 'molasses',\n",
       " 199: 'shredded cabbage',\n",
       " 200: 'sweet and sour sauce',\n",
       " 201: 'breakfast sausages',\n",
       " 202: 'cheese',\n",
       " 203: 'flour tortillas',\n",
       " 204: 'large eggs',\n",
       " 205: 'boiling water',\n",
       " 206: 'andouille sausage',\n",
       " 207: 'bacon',\n",
       " 208: 'cajun seasoning',\n",
       " 209: 'celery',\n",
       " 210: 'fresh parsley',\n",
       " 211: 'ground red pepper',\n",
       " 212: 'hot sauce',\n",
       " 213: 'peanut oil',\n",
       " 214: 'potatoes',\n",
       " 215: 'shrimp',\n",
       " 216: 'onion powder',\n",
       " 217: 'chile powder',\n",
       " 218: 'chinese chives',\n",
       " 219: 'firm tofu',\n",
       " 220: 'fish sauce',\n",
       " 221: 'green papaya',\n",
       " 222: 'lime wedges',\n",
       " 223: 'palm sugar',\n",
       " 224: 'sliced chicken',\n",
       " 225: 'tamarind paste',\n",
       " 226: 'turnips',\n",
       " 227: 'crushed garlic',\n",
       " 228: 'diced tomatoes',\n",
       " 229: 'dried basil',\n",
       " 230: 'green onions',\n",
       " 231: 'white sugar',\n",
       " 232: 'Johnsonville Andouille Dinner Sausage',\n",
       " 233: 'creole seasoning',\n",
       " 234: 'hot pepper sauce',\n",
       " 235: 'jambalaya rice mix',\n",
       " 236: 'parsley',\n",
       " 237: 'worcestershire sauce',\n",
       " 238: 'bread slices',\n",
       " 239: 'great northern beans',\n",
       " 240: 'sage leaves',\n",
       " 241: 'chicken thighs',\n",
       " 242: 'chinese five-spice powder',\n",
       " 243: 'star anise',\n",
       " 244: 'black-eyed peas',\n",
       " 245: 'cream cheese',\n",
       " 246: 'shredded cheddar cheese',\n",
       " 247: 'tortilla chips',\n",
       " 248: 'apple cider',\n",
       " 249: 'bay leaves',\n",
       " 250: 'chicken stock',\n",
       " 251: 'collard greens',\n",
       " 252: 'freshly ground pepper',\n",
       " 253: 'ham hock',\n",
       " 254: 'large garlic cloves',\n",
       " 255: 'light brown sugar',\n",
       " 256: 'rib',\n",
       " 257: 'Oscar Mayer Deli Fresh Smoked Ham',\n",
       " 258: 'giardiniera',\n",
       " 259: 'hoagie rolls',\n",
       " 260: 'mozzarella cheese',\n",
       " 261: 'pepperoni',\n",
       " 262: 'salami',\n",
       " 263: 'club soda',\n",
       " 264: 'ice cubes',\n",
       " 265: 'turbinado',\n",
       " 266: 'white rum',\n",
       " 267: 'canned black beans',\n",
       " 268: 'enchilada sauce',\n",
       " 269: 'green pepper',\n",
       " 270: 'picante sauce',\n",
       " 271: 'shredded lettuce',\n",
       " 272: 'sour cream',\n",
       " 273: 'cumin seed',\n",
       " 274: 'curry powder',\n",
       " 275: 'salmon fillets',\n",
       " 276: 'sauce',\n",
       " 277: 'serrano chile',\n",
       " 278: 'chicken breast halves',\n",
       " 279: 'chopped onion',\n",
       " 280: 'lettuce leaves',\n",
       " 281: 'frosting',\n",
       " 282: 'mandarin oranges',\n",
       " 283: 'orange liqueur',\n",
       " 284: 'yellow cake mix',\n",
       " 285: 'fennel bulb',\n",
       " 286: 'grapefruit juice',\n",
       " 287: 'lemon olive oil',\n",
       " 288: 'bacon drippings',\n",
       " 289: 'buttermilk',\n",
       " 290: 'white cornmeal',\n",
       " 291: 'crab boil',\n",
       " 292: 'crawfish',\n",
       " 293: 'old bay seasoning',\n",
       " 294: 'corn oil',\n",
       " 295: 'glass noodles',\n",
       " 296: 'mirin',\n",
       " 297: 'napa cabbage',\n",
       " 298: 'sake',\n",
       " 299: 'shiitake',\n",
       " 300: 'sirloin',\n",
       " 301: 'extra sharp cheddar cheese',\n",
       " 302: 'monterey jack',\n",
       " 303: 'quickcooking grits',\n",
       " 304: 'whipping cream',\n",
       " 305: 'basil',\n",
       " 306: 'crushed red pepper',\n",
       " 307: 'dry white wine',\n",
       " 308: 'finely chopped onion',\n",
       " 309: 'mussels',\n",
       " 310: 'baking soda',\n",
       " 311: 'honey',\n",
       " 312: 'unsalted butter',\n",
       " 313: 'lemon',\n",
       " 314: 'pesto',\n",
       " 315: 'white wine',\n",
       " 316: 'allspice',\n",
       " 317: 'bread crumbs',\n",
       " 318: 'cold water',\n",
       " 319: 'curry',\n",
       " 320: 'fresh thyme',\n",
       " 321: 'ground beef',\n",
       " 322: 'low sodium beef broth',\n",
       " 323: 'pickapeppa sauce',\n",
       " 324: 'chopped walnuts',\n",
       " 325: 'matcha green tea powder',\n",
       " 326: 'melted butter',\n",
       " 327: 'coarse salt',\n",
       " 328: 'fenugreek',\n",
       " 329: 'urad dal',\n",
       " 330: 'white rice',\n",
       " 331: 'part-skim mozzarella cheese',\n",
       " 332: 'pizza crust',\n",
       " 333: '1% low-fat milk',\n",
       " 334: 'leeks',\n",
       " 335: 'low-fat sour cream',\n",
       " 336: 'yukon gold potatoes',\n",
       " 337: 'dry sherry',\n",
       " 338: 'golden brown sugar',\n",
       " 339: 'lemongrass',\n",
       " 340: 'peanut sauce',\n",
       " 341: 'spareribs',\n",
       " 342: 'tamari soy sauce',\n",
       " 343: 'unsweetened coconut milk',\n",
       " 344: 'chicken legs',\n",
       " 345: 'ghee',\n",
       " 346: 'ground turmeric',\n",
       " 347: 'tomato paste',\n",
       " 348: 'chillies',\n",
       " 349: 'cracked black pepper',\n",
       " 350: 'dried thyme',\n",
       " 351: 'ginger',\n",
       " 352: 'minced beef',\n",
       " 353: 'boneless pork loin',\n",
       " 354: 'crushed tomatoes',\n",
       " 355: 'fresh rosemary',\n",
       " 356: 'pappardelle',\n",
       " 357: 'coconut milk',\n",
       " 358: 'eggplant',\n",
       " 359: 'Gochujang base',\n",
       " 360: 'Taiwanese bok choy',\n",
       " 361: 'jasmine rice',\n",
       " 362: 'rice vinegar',\n",
       " 363: 'top round steak',\n",
       " 364: 'large egg yolks',\n",
       " 365: 'vanilla',\n",
       " 366: 'gingersnap',\n",
       " 367: 'ground nutmeg',\n",
       " 368: 'maple syrup',\n",
       " 369: 'marshmallow creme',\n",
       " 370: 'orange juice concentrate',\n",
       " 371: 'pumpkin purГ©e',\n",
       " 372: 'toasted pecans',\n",
       " 373: 'brown sugar',\n",
       " 374: 'dijon mustard',\n",
       " 375: 'manchego cheese',\n",
       " 376: 'membrillo',\n",
       " 377: 'serrano ham',\n",
       " 378: 'sourdough',\n",
       " 379: 'Sriracha',\n",
       " 380: 'burger buns',\n",
       " 381: 'chickpea flour',\n",
       " 382: 'chickpeas',\n",
       " 383: 'greens',\n",
       " 384: 'ground flaxseed',\n",
       " 385: 'panko breadcrumbs',\n",
       " 386: 'tahini',\n",
       " 387: 'ground cardamom',\n",
       " 388: 'ground cloves',\n",
       " 389: 'whole nutmegs',\n",
       " 390: 'Italian bread',\n",
       " 391: 'balsamic vinegar',\n",
       " 392: 'penne pasta',\n",
       " 393: 'roasted red peppers',\n",
       " 394: 'sausage casings',\n",
       " 395: 'shredded mozzarella cheese',\n",
       " 396: 'saffron',\n",
       " 397: 'warm water',\n",
       " 398: 'cayenne',\n",
       " 399: 'couscous',\n",
       " 400: 'fat',\n",
       " 401: 'fat skimmed chicken broth',\n",
       " 402: 'harissa',\n",
       " 403: 'pitted kalamata olives',\n",
       " 404: 'preserved lemon',\n",
       " 405: 'espresso',\n",
       " 406: 'ice',\n",
       " 407: 'sweetened condensed milk',\n",
       " 408: 'fresh asparagus',\n",
       " 409: 'chuck',\n",
       " 410: 'crumbled blue cheese',\n",
       " 411: 'hamburger buns',\n",
       " 412: 'okra pods',\n",
       " 413: 'pecans',\n",
       " 414: 'watercress',\n",
       " 415: 'food colouring',\n",
       " 416: 'self rising flour',\n",
       " 417: 'softened butter',\n",
       " 418: 'Fuyu persimmons',\n",
       " 419: 'Madeira',\n",
       " 420: 'demi-glace',\n",
       " 421: 'egg bread',\n",
       " 422: 'foie gras',\n",
       " 423: 'sherry vinegar',\n",
       " 424: 'white bread',\n",
       " 425: 'amchur',\n",
       " 426: 'black mustard seeds',\n",
       " 427: 'coriander',\n",
       " 428: 'fenugreek leaves',\n",
       " 429: 'fingerling potatoes',\n",
       " 430: 'ramps',\n",
       " 431: 'black peppercorns',\n",
       " 432: 'cardamom pods',\n",
       " 433: 'cinnamon sticks',\n",
       " 434: 'coriander seeds',\n",
       " 435: 'asiago',\n",
       " 436: 'sweet onion',\n",
       " 437: 'whole wheat pasta',\n",
       " 438: 'chestnut flour',\n",
       " 439: 'chestnuts',\n",
       " 440: 'coffee ice cream',\n",
       " 441: 'granulated sugar',\n",
       " 442: 'mascarpone',\n",
       " 443: 'rum',\n",
       " 444: 'semisweet chocolate',\n",
       " 445: 'whole milk ricotta cheese',\n",
       " 446: 'baby spinach leaves',\n",
       " 447: 'chopped garlic',\n",
       " 448: 'large shrimp',\n",
       " 449: 'barley',\n",
       " 450: 'cooked ham',\n",
       " 451: 'grated nutmeg',\n",
       " 452: 'starchy potatoes',\n",
       " 453: 'sirloin tip roast',\n",
       " 454: 'steak',\n",
       " 455: 'vegetable shortening',\n",
       " 456: 'caul fat',\n",
       " 457: 'finely chopped fresh parsley',\n",
       " 458: 'vinegar',\n",
       " 459: 'heavy whipping cream',\n",
       " 460: 'instant espresso powder',\n",
       " 461: 'kahlГєa',\n",
       " 462: 'whole milk',\n",
       " 463: 'epazote',\n",
       " 464: 'hominy',\n",
       " 465: 'oregano',\n",
       " 466: 'pepitas',\n",
       " 467: 'radishes',\n",
       " 468: 'tomatillos',\n",
       " 469: 'white onion',\n",
       " 470: 'catfish fillets',\n",
       " 471: 'basmati rice',\n",
       " 472: 'cauliflower',\n",
       " 473: 'cumin',\n",
       " 474: 'blood orange',\n",
       " 475: 'low salt chicken broth',\n",
       " 476: 'white wine vinegar',\n",
       " 477: 'chopped green bell pepper',\n",
       " 478: 'english muffins, split and toasted',\n",
       " 479: 'shredded carrots',\n",
       " 480: 'tomato sauce',\n",
       " 481: 'vegetable oil cooking spray',\n",
       " 482: 'prawns',\n",
       " 483: 'rice flour',\n",
       " 484: 'seasoning salt',\n",
       " 485: 'evaporated milk',\n",
       " 486: 'lasagna noodles',\n",
       " 487: 'ranch dressing',\n",
       " 488: 'boneless chop pork',\n",
       " 489: 'minced garlic',\n",
       " 490: 'red wine vinegar',\n",
       " 491: 'chile paste',\n",
       " 492: 'dark sesame oil',\n",
       " 493: 'lower sodium soy sauce',\n",
       " 494: 'pearl barley',\n",
       " 495: 'raisins',\n",
       " 496: 'slivered almonds',\n",
       " 497: 'chocolate bars',\n",
       " 498: 'cinnamon graham crackers',\n",
       " 499: 'marshmallows',\n",
       " 500: 'chopped celery',\n",
       " 501: 'cod',\n",
       " 502: 'dry bread crumbs',\n",
       " 503: 'minced onion',\n",
       " 504: 'mustard powder',\n",
       " 505: 'brewed tea',\n",
       " 506: 'candied orange peel',\n",
       " 507: 'dark brown sugar',\n",
       " 508: 'dried currants',\n",
       " 509: 'golden raisins',\n",
       " 510: 'grated lemon zest',\n",
       " 511: 'mace',\n",
       " 512: 'margarine',\n",
       " 513: 'rum extract',\n",
       " 514: 'baby bok choy',\n",
       " 515: 'black bean sauce',\n",
       " 516: 'noodles',\n",
       " 517: 'red potato',\n",
       " 518: 'chinese rice wine',\n",
       " 519: 'black beans',\n",
       " 520: 'frozen corn',\n",
       " 521: 'quinoa',\n",
       " 522: 'vegetable broth',\n",
       " 523: 'yellow bell pepper',\n",
       " 524: 'duck',\n",
       " 525: 'orange',\n",
       " 526: 'dried mint flakes',\n",
       " 527: 'feta cheese',\n",
       " 528: 'ground lamb',\n",
       " 529: 'penne rigate',\n",
       " 530: 'ras el hanout',\n",
       " 531: '2% reduced-fat milk',\n",
       " 532: 'large egg whites',\n",
       " 533: 'chicken leg quarters',\n",
       " 534: 'cucumber',\n",
       " 535: 'long-grain rice',\n",
       " 536: 'napa cabbage leaves',\n",
       " 537: 'baby potatoes',\n",
       " 538: 'crusty bread',\n",
       " 539: 'smoked streaky bacon',\n",
       " 540: 'ground allspice',\n",
       " 541: 'hibiscus',\n",
       " 542: 'piloncillo',\n",
       " 543: 'fresh oregano',\n",
       " 544: 'chocolate covered coffee beans',\n",
       " 545: 'chocolate morsels',\n",
       " 546: 'cream sweeten whip',\n",
       " 547: 'instant espresso granules',\n",
       " 548: 'pound cake',\n",
       " 549: 'unflavored gelatin',\n",
       " 550: 'arborio rice',\n",
       " 551: 'asparagus spears',\n",
       " 552: 'baby carrots',\n",
       " 553: 'fresh basil leaves',\n",
       " 554: 'frozen peas',\n",
       " 555: 'pinenuts',\n",
       " 556: 'yellow crookneck squash',\n",
       " 557: 'zucchini',\n",
       " 558: 'chipotles in adobo',\n",
       " 559: 'cilantro leaves',\n",
       " 560: 'instant white rice',\n",
       " 561: 'lime zest',\n",
       " 562: 'red beans',\n",
       " 563: 'skinless chicken thighs',\n",
       " 564: 'Mexican cheese blend',\n",
       " 565: 'chunky salsa',\n",
       " 566: 'condensed cream of chicken soup',\n",
       " 567: 'condensed cream of mushroom soup',\n",
       " 568: 'lean ground beef',\n",
       " 569: 'taco seasoning mix',\n",
       " 570: 'chorizo spanish',\n",
       " 571: 'dark rum',\n",
       " 572: 'pineapple juice',\n",
       " 573: 'daikon',\n",
       " 574: 'goji berries',\n",
       " 575: 'oysters',\n",
       " 576: 'pork ribs',\n",
       " 577: 'salsa',\n",
       " 578: 'demerara sugar',\n",
       " 579: 'egg whites',\n",
       " 580: 'frozen pastry puff sheets',\n",
       " 581: 'fruit',\n",
       " 582: 'mixed spice',\n",
       " 583: 'brown mustard seeds',\n",
       " 584: 'chili oil',\n",
       " 585: 'parsnips',\n",
       " 586: 'basil leaves',\n",
       " 587: 'chicken tenderloin',\n",
       " 588: 'chili flakes',\n",
       " 589: 'frozen broad beans',\n",
       " 590: 'kecap manis',\n",
       " 591: 'thai green curry paste',\n",
       " 592: 'unsalted cashews',\n",
       " 593: 'condensed cheddar cheese soup',\n",
       " 594: 'creole style seasoning',\n",
       " 595: 'grated jack cheese',\n",
       " 596: 'refried beans',\n",
       " 597: 'egg yolks',\n",
       " 598: 'asparagus',\n",
       " 599: 'cheese tortellini',\n",
       " 600: 'cherry tomatoes',\n",
       " 601: 'fresh leav spinach',\n",
       " 602: 'navy beans',\n",
       " 603: 'granny smith apples',\n",
       " 604: 'nonfat milk',\n",
       " 605: 'lard',\n",
       " 606: 'fresh fava bean',\n",
       " 607: 'italian sausage',\n",
       " 608: 'pasta sheets',\n",
       " 609: 'pecorino romano cheese',\n",
       " 610: 'Flora Cuisine',\n",
       " 611: 'curry paste',\n",
       " 612: 'low-fat natural yogurt',\n",
       " 613: 'spinach leaves',\n",
       " 614: 'bone-in chicken breasts',\n",
       " 615: 'dried chile peppers',\n",
       " 616: 'white hominy',\n",
       " 617: 'chocolate instant pudding',\n",
       " 618: 'frozen whipped topping',\n",
       " 619: 'graham cracker crumbs',\n",
       " 620: 'instant butterscotch pudding mix',\n",
       " 621: 'fennel seeds',\n",
       " 622: 'kalamata',\n",
       " 623: 'celery ribs',\n",
       " 624: 'ham',\n",
       " 625: 'kielbasa',\n",
       " 626: 'capsicum',\n",
       " 627: 'green mango',\n",
       " 628: 'moong dal',\n",
       " 629: 'dried shiitake mushrooms',\n",
       " 630: 'gari',\n",
       " 631: 'short-grain rice',\n",
       " 632: 'toasted sesame oil',\n",
       " 633: 'tofu',\n",
       " 634: 'Turkish bay leaves',\n",
       " 635: 'dried chickpeas',\n",
       " 636: 'semolina',\n",
       " 637: 'vine ripened tomatoes',\n",
       " 638: 'angel food cake',\n",
       " 639: 'bittersweet chocolate',\n",
       " 640: 'fat free yogurt',\n",
       " 641: 'instant espresso',\n",
       " 642: 'skim milk',\n",
       " 643: 'unsweetened cocoa powder',\n",
       " 644: 'organic chicken',\n",
       " 645: 'anise',\n",
       " 646: 'calimyrna figs',\n",
       " 647: 'clove',\n",
       " 648: 'orange blossom honey',\n",
       " 649: 'plain whole-milk yogurt',\n",
       " 650: 'zinfandel',\n",
       " 651: 'beef stock',\n",
       " 652: 'chuck roast',\n",
       " 653: 'reduced sodium soy sauce',\n",
       " 654: 'Corn Flakes Cereal',\n",
       " 655: 'coleslaw',\n",
       " 656: 'garlic salt',\n",
       " 657: 'asian pear',\n",
       " 658: 'chili paste',\n",
       " 659: 'kiwi',\n",
       " 660: 'red pepper flakes',\n",
       " 661: 'chees fresco queso',\n",
       " 662: 'swiss chard',\n",
       " 663: 'veal cutlets',\n",
       " 664: 'pasta',\n",
       " 665: 'extra-lean ground beef',\n",
       " 666: 'wakame',\n",
       " 667: 'nonstick spray',\n",
       " 668: 'peeled fresh ginger',\n",
       " 669: 'scallion greens',\n",
       " 670: 'wonton wrappers',\n",
       " 671: 'anchovy fillets',\n",
       " 672: 'chicken bones',\n",
       " 673: 'chopped fresh thyme',\n",
       " 674: 'crusty rolls',\n",
       " 675: 'green bell pepper, slice',\n",
       " 676: 'almond flour',\n",
       " 677: 'coconut aminos',\n",
       " 678: 'coconut oil',\n",
       " 679: 'boneless skinless chicken breast halves',\n",
       " 680: 'clams',\n",
       " 681: 'green peas',\n",
       " 682: 'seasoned rice wine vinegar',\n",
       " 683: 'snow peas',\n",
       " 684: 'artichoke hearts',\n",
       " 685: 'hard-boiled egg',\n",
       " 686: 'coriander powder',\n",
       " 687: 'maida flour',\n",
       " 688: 'stuffing',\n",
       " 689: 'pita bread rounds',\n",
       " 690: 'grated GruyГЁre cheese',\n",
       " 691: 'button mushrooms',\n",
       " 692: 'red miso',\n",
       " 693: 'silken tofu',\n",
       " 694: 'udon',\n",
       " 695: 'pickled jalapenos',\n",
       " 696: 'shredded Monterey Jack cheese',\n",
       " 697: 'whole peeled tomatoes',\n",
       " 698: 'broccoli rabe',\n",
       " 699: 'fontina',\n",
       " 700: 'parmigiano reggiano cheese',\n",
       " 701: 'peppercorns',\n",
       " 702: 'thai chile',\n",
       " 703: 'cream of chicken soup',\n",
       " 704: 'frozen peas and carrots',\n",
       " 705: 'low sodium chicken broth',\n",
       " 706: 'refrigerated piecrusts',\n",
       " 707: 'frozen chopped spinach',\n",
       " 708: 'manicotti shells',\n",
       " 709: 'olive oil flavored cooking spray',\n",
       " 710: 'part-skim ricotta cheese',\n",
       " 711: 'pasta sauce',\n",
       " 712: 'chicken bouillon',\n",
       " 713: 'fettucine',\n",
       " 714: 'parmesan cheese',\n",
       " 715: 'chili paste with garlic',\n",
       " 716: 'fresh veget',\n",
       " 717: 'condiments',\n",
       " 718: 'guacamole',\n",
       " 719: 'pork shoulder roast',\n",
       " 720: 'salsa verde',\n",
       " 721: 'taco sauce',\n",
       " 722: 'tortillas',\n",
       " 723: 'almonds',\n",
       " 724: 'pistachios',\n",
       " 725: 'rose petals',\n",
       " 726: 'chicken wings',\n",
       " 727: 'adobo sauce',\n",
       " 728: 'ancho powder',\n",
       " 729: 'chicken',\n",
       " 730: 'chiles',\n",
       " 731: 'chipotle chile powder',\n",
       " 732: 'drumstick',\n",
       " 733: 'green chile',\n",
       " 734: 'peeled tomatoes',\n",
       " 735: 'Old El Paso Flour Tortillas',\n",
       " 736: 'beef rib short',\n",
       " 737: 'beer',\n",
       " 738: 'chili sauce',\n",
       " 739: 'cranberry sauce',\n",
       " 740: 'dashi',\n",
       " 741: 'miso paste',\n",
       " 742: 'seaweed',\n",
       " 743: 'fresh dill',\n",
       " 744: 'kefalotyri',\n",
       " 745: 'myzithra',\n",
       " 746: 'phyllo',\n",
       " 747: 'yoghurt',\n",
       " 748: 'cracker crumbs',\n",
       " 749: 'okra',\n",
       " 750: 'Philadelphia Cream Cheese',\n",
       " 751: 'crab meat',\n",
       " 752: 'savoy cabbage',\n",
       " 753: 'boneless chicken thighs',\n",
       " 754: 'potato starch',\n",
       " 755: 'cake flour',\n",
       " 756: 'cream of tartar',\n",
       " 757: 'ricotta cheese',\n",
       " 758: 'semi-sweet chocolate morsels',\n",
       " 759: 'serrano',\n",
       " 760: 'hazelnut oil',\n",
       " 761: 'hazelnuts',\n",
       " 762: 'steel-cut oats',\n",
       " 763: 'sweet cherries',\n",
       " 764: 'fire roasted diced tomatoes',\n",
       " 765: 'red kidnei beans, rins and drain',\n",
       " 766: 'Japanese soy sauce',\n",
       " 767: 'black sesame seeds',\n",
       " 768: 'fresh salmon',\n",
       " 769: 'wasabi paste',\n",
       " 770: 'cremini mushrooms',\n",
       " 771: 'dried porcini mushrooms',\n",
       " 772: 'dry red wine',\n",
       " 773: 'fat free less sodium beef broth',\n",
       " 774: 'hot water',\n",
       " 775: 'celery salt',\n",
       " 776: 'creole mustard',\n",
       " 777: 'prepared horseradish',\n",
       " 778: 'shrimp and crab boil seasoning',\n",
       " 779: 'apples',\n",
       " 780: 'butternut squash',\n",
       " 781: 'country ham',\n",
       " 782: 'turnip greens',\n",
       " 783: 'frozen edamame beans',\n",
       " 784: 'whole kernel corn, drain',\n",
       " 785: 'fresh coriander',\n",
       " 786: 'instant yeast',\n",
       " 787: 'kalonji',\n",
       " 788: 'salted butter',\n",
       " 789: 'strong white bread flour',\n",
       " 790: 'sunflower oil',\n",
       " 791: 'cooked white rice',\n",
       " 792: 'mixed mushrooms',\n",
       " 793: 'vegetables',\n",
       " 794: 'wood ear mushrooms',\n",
       " 795: 'bell pepper',\n",
       " 796: 'boneless skinless chicken breasts',\n",
       " 797: 'shredded cheese',\n",
       " 798: 'taco seasoning',\n",
       " 799: 'baking apples',\n",
       " 800: 'cane sugar',\n",
       " 801: 'eau de vie',\n",
       " 802: 'leaves',\n",
       " 803: 'salt and ground black pepper',\n",
       " 804: 'chopped green chilies',\n",
       " 805: 'sliced black olives',\n",
       " 806: 'Guinness Lager',\n",
       " 807: 'dried rosemary',\n",
       " 808: 'lean steak',\n",
       " 809: 'rosemary leaves',\n",
       " 810: 'chinese cabbage',\n",
       " 811: 'dipping sauces',\n",
       " 812: 'lean ground pork',\n",
       " 813: 'water chestnuts, drained and chopped',\n",
       " 814: 'cheese slices',\n",
       " 815: 'chorizo',\n",
       " 816: 'french bread',\n",
       " 817: 'pico de gallo',\n",
       " 818: 'dried dillweed',\n",
       " 819: 'lamb',\n",
       " 820: 'plain yogurt',\n",
       " 821: 'red wine',\n",
       " 822: 'kale',\n",
       " 823: 'Mexican beer',\n",
       " 824: 'boneless beef short ribs',\n",
       " 825: 'guajillo',\n",
       " 826: 'fresh spinach',\n",
       " 827: 'saffron threads',\n",
       " 828: 'banana squash',\n",
       " 829: 'cannellini beans',\n",
       " 830: 'shortening',\n",
       " 831: 'Franks Hot Sauce',\n",
       " 832: 'light pancake syrup',\n",
       " 833: 'pears',\n",
       " 834: 'puff pastry',\n",
       " 835: 'apple cider vinegar',\n",
       " 836: 'bone in chicken thighs',\n",
       " 837: 'fresh bay leaves',\n",
       " 838: 'mushroom powder',\n",
       " 839: 'cilantro',\n",
       " 840: 'skinless chicken breasts',\n",
       " 841: 'fava beans',\n",
       " 842: 'fresh tarragon',\n",
       " 843: 'light sour cream',\n",
       " 844: 'kidney beans',\n",
       " 845: 'orzo pasta',\n",
       " 846: 'dri leav thyme',\n",
       " 847: 'jumbo shrimp',\n",
       " 848: 'sticky rice',\n",
       " 849: 'wasabi',\n",
       " 850: 'white sesame seeds',\n",
       " 851: 'chopped almonds',\n",
       " 852: 'baby arugula',\n",
       " 853: 'margarita mix',\n",
       " 854: 'tequila',\n",
       " 855: 'corn syrup',\n",
       " 856: 'crust',\n",
       " 857: 'grits',\n",
       " 858: 'pecan halves',\n",
       " 859: 'active dry yeast',\n",
       " 860: 'blackberries',\n",
       " 861: 'fresh mint',\n",
       " 862: 'luke warm water',\n",
       " 863: 'superfine sugar',\n",
       " 864: 'whiskey',\n",
       " 865: 'chicken drumsticks',\n",
       " 866: 'jamaican jerk season',\n",
       " 867: 'pineapple preserves',\n",
       " 868: 'goat cheese',\n",
       " 869: 'pitas',\n",
       " 870: 'gingerroot',\n",
       " 871: 'hot bean paste',\n",
       " 872: 'rib eye steaks',\n",
       " 873: 'cream',\n",
       " 874: 'diced bell pepper',\n",
       " 875: 'medjool date',\n",
       " 876: 'smoked paprika',\n",
       " 877: 'reduced fat milk',\n",
       " 878: 'smoked trout',\n",
       " 879: 'garlic puree',\n",
       " 880: 'green cardamom pods',\n",
       " 881: 'tomato purГ©e',\n",
       " 882: 'chile sauce',\n",
       " 883: 'crema mexicana',\n",
       " 884: 'pickled jalapeno peppers',\n",
       " 885: 'queso fresco',\n",
       " 886: 'romaine lettuce leaves',\n",
       " 887: 'Boston lettuce',\n",
       " 888: 'peanut butter',\n",
       " 889: 'persian cucumber',\n",
       " 890: 'rice paper',\n",
       " 891: 'Korean chile flakes',\n",
       " 892: 'boneless chicken breast',\n",
       " 893: 'cashew nuts',\n",
       " 894: 'cinnamon',\n",
       " 895: 'ginger paste',\n",
       " 896: 'kashmiri chile',\n",
       " 897: 'kasuri methi',\n",
       " 898: 'mustard oil',\n",
       " 899: 'red chile powder',\n",
       " 900: 'tomato ketchup',\n",
       " 901: 'pecan pie',\n",
       " 902: 'corn kernels',\n",
       " 903: 'grouper',\n",
       " 904: 'tostada shells',\n",
       " 905: 'ricotta salata',\n",
       " 906: 'grana padano',\n",
       " 907: 'pecorino cheese',\n",
       " 908: 'bamboo shoots',\n",
       " 909: 'corn',\n",
       " 910: 'galangal',\n",
       " 911: 'green curry paste',\n",
       " 912: 'kaffir lime leaves',\n",
       " 913: 'soba noodles',\n",
       " 914: 'bourbon whiskey',\n",
       " 915: 'chopped nuts',\n",
       " 916: 'ketchup',\n",
       " 917: 'rhubarb',\n",
       " 918: 'liquid',\n",
       " 919: 'safflower oil',\n",
       " 920: 'store bought low sodium chicken stock',\n",
       " 921: 'cardamom',\n",
       " 922: 'jeera',\n",
       " 923: 'crema',\n",
       " 924: 'salmon steaks',\n",
       " 925: 'jam',\n",
       " 926: 'free range egg',\n",
       " 927: 'karashi',\n",
       " 928: 'spring onions',\n",
       " 929: 'diced celery',\n",
       " 930: 'jicama',\n",
       " 931: 'water chestnuts',\n",
       " 932: 'dinner rolls',\n",
       " 933: 'shredded sharp cheddar cheese',\n",
       " 934: 'cactus pad',\n",
       " 935: 'boiling potatoes',\n",
       " 936: 'cauliflower flowerets',\n",
       " 937: 'fenugreek seeds',\n",
       " 938: 'baby spinach',\n",
       " 939: 'tuna steaks',\n",
       " 940: 'spices',\n",
       " 941: 'octopuses',\n",
       " 942: 'small red potato',\n",
       " 943: 'smoked gouda',\n",
       " 944: 'vidalia onion',\n",
       " 945: 'fat-free refried beans',\n",
       " 946: 'fresh lime',\n",
       " 947: 'low-fat milk',\n",
       " 948: 'tuna',\n",
       " 949: 'ground white pepper',\n",
       " 950: 'BertolliВ® Classico Olive Oil',\n",
       " 951: 'bacon, crisp-cooked and crumbled',\n",
       " 952: 'bertolli vineyard premium collect marinara with burgundi wine sauc',\n",
       " 953: 'bread crumb fresh',\n",
       " 954: 'margarita salt',\n",
       " 955: '(    oz.) tomato sauce',\n",
       " 956: 'beef',\n",
       " 957: 'ground veal',\n",
       " 958: 'italian seasoning mix',\n",
       " 959: 'solid pack pumpkin',\n",
       " 960: 'fresh thyme leaves',\n",
       " 961: 'grapeseed oil',\n",
       " 962: 'organic sugar',\n",
       " 963: 'whole grain thin spaghetti',\n",
       " 964: 'dried cherry',\n",
       " 965: 'straw mushrooms',\n",
       " 966: 'haricots verts',\n",
       " 967: 'vidalia',\n",
       " 968: 'wax beans',\n",
       " 969: 'prosciutto',\n",
       " 970: 'romano cheese',\n",
       " 971: 'cooked shrimp',\n",
       " 972: 'crabmeat',\n",
       " 973: 'blueberri preserv',\n",
       " 974: 'chipotle peppers',\n",
       " 975: 'coffee beans',\n",
       " 976: 'filet',\n",
       " 977: 'green peppercorns',\n",
       " 978: 'pork loin chops',\n",
       " 979: 'strawberries',\n",
       " 980: 'thyme leaves',\n",
       " 981: 'vegan margarine',\n",
       " 982: 'pork shoulder',\n",
       " 983: 'parsley leaves',\n",
       " 984: 'fresh raspberries',\n",
       " 985: 'frozen whole kernel corn',\n",
       " 986: 'torn romain lettuc leav',\n",
       " 987: 'wish bone guacamol ranch dress',\n",
       " 988: 'colby cheese',\n",
       " 989: 'half & half',\n",
       " 990: 'mexican chorizo',\n",
       " 991: 'russet potatoes',\n",
       " 992: 'canned low sodium chicken broth',\n",
       " 993: 'dry vermouth',\n",
       " 994: 'roasted salted cashews',\n",
       " 995: 'fresh lemon',\n",
       " 996: 'paneer cheese',\n",
       " 997: 'toasted sesame seeds',\n",
       " 998: 'Thai fish sauce',\n",
       " 999: 'english cucumber',\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Dictionary in module gensim.corpora.dictionary object:\n",
      "\n",
      "class Dictionary(gensim.utils.SaveLoad, collections.abc.Mapping)\n",
      " |  Dictionary(documents=None, prune_at=2000000)\n",
      " |  \n",
      " |  Dictionary encapsulates the mapping between normalized words and their integer ids.\n",
      " |  \n",
      " |  Notable instance attributes:\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  token2id : dict of (str, int)\n",
      " |      token -> tokenId.\n",
      " |  id2token : dict of (int, str)\n",
      " |      Reverse mapping for token2id, initialized in a lazy manner to save memory (not created until needed).\n",
      " |  cfs : dict of (int, int)\n",
      " |      Collection frequencies: token_id -> how many instances of this token are contained in the documents.\n",
      " |  dfs : dict of (int, int)\n",
      " |      Document frequencies: token_id -> how many documents contain this token.\n",
      " |  num_docs : int\n",
      " |      Number of documents processed.\n",
      " |  num_pos : int\n",
      " |      Total number of corpus positions (number of processed words).\n",
      " |  num_nnz : int\n",
      " |      Total number of non-zeroes in the BOW matrix (sum of the number of unique\n",
      " |      words per document over the entire corpus).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dictionary\n",
      " |      gensim.utils.SaveLoad\n",
      " |      collections.abc.Mapping\n",
      " |      collections.abc.Collection\n",
      " |      collections.abc.Sized\n",
      " |      collections.abc.Iterable\n",
      " |      collections.abc.Container\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, tokenid)\n",
      " |      Get the string token that corresponds to `tokenid`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tokenid : int\n",
      " |          Id of token.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Token corresponding to `tokenid`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If this Dictionary doesn't contain such `tokenid`.\n",
      " |  \n",
      " |  __init__(self, documents=None, prune_at=2000000)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      documents : iterable of iterable of str, optional\n",
      " |          Documents to be used to initialize the mapping and collect corpus statistics.\n",
      " |      prune_at : int, optional\n",
      " |          Dictionary will try to keep no more than `prune_at` words in its mapping, to limit its RAM\n",
      " |          footprint, the correctness is not guaranteed.\n",
      " |          Use :meth:`~gensim.corpora.dictionary.Dictionary.filter_extremes` to perform proper filtering.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>>\n",
      " |          >>> texts = [['human', 'interface', 'computer']]\n",
      " |          >>> dct = Dictionary(texts)  # initialize a Dictionary\n",
      " |          >>> dct.add_documents([[\"cat\", \"say\", \"meow\"], [\"dog\"]])  # add more document (extend the vocabulary)\n",
      " |          >>> dct.doc2bow([\"dog\", \"computer\", \"non_existent_word\"])\n",
      " |          [(0, 1), (6, 1)]\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Iterate over all tokens.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Get number of stored tokens.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          Number of stored tokens.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  add_documents(self, documents, prune_at=2000000)\n",
      " |      Update dictionary from a collection of `documents`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      documents : iterable of iterable of str\n",
      " |          Input corpus. All tokens should be already **tokenized and normalized**.\n",
      " |      prune_at : int, optional\n",
      " |          Dictionary will try to keep no more than `prune_at` words in its mapping, to limit its RAM\n",
      " |          footprint, the correctness is not guaranteed.\n",
      " |          Use :meth:`~gensim.corpora.dictionary.Dictionary.filter_extremes` to perform proper filtering.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>>\n",
      " |          >>> corpus = [\"máma mele maso\".split(), \"ema má máma\".split()]\n",
      " |          >>> dct = Dictionary(corpus)\n",
      " |          >>> len(dct)\n",
      " |          5\n",
      " |          >>> dct.add_documents([[\"this\", \"is\", \"sparta\"], [\"just\", \"joking\"]])\n",
      " |          >>> len(dct)\n",
      " |          10\n",
      " |  \n",
      " |  compactify(self)\n",
      " |      Assign new word ids to all words, shrinking any gaps.\n",
      " |  \n",
      " |  doc2bow(self, document, allow_update=False, return_missing=False)\n",
      " |      Convert `document` into the bag-of-words (BoW) format = list of `(token_id, token_count)` tuples.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      document : list of str\n",
      " |          Input document.\n",
      " |      allow_update : bool, optional\n",
      " |          Update self, by adding new tokens from `document` and updating internal corpus statistics.\n",
      " |      return_missing : bool, optional\n",
      " |          Return missing tokens (tokens present in `document` but not in self) with frequencies?\n",
      " |      \n",
      " |      Return\n",
      " |      ------\n",
      " |      list of (int, int)\n",
      " |          BoW representation of `document`.\n",
      " |      list of (int, int), dict of (str, int)\n",
      " |          If `return_missing` is True, return BoW representation of `document` + dictionary with missing\n",
      " |          tokens and their frequencies.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>> dct = Dictionary([\"máma mele maso\".split(), \"ema má máma\".split()])\n",
      " |          >>> dct.doc2bow([\"this\", \"is\", \"máma\"])\n",
      " |          [(2, 1)]\n",
      " |          >>> dct.doc2bow([\"this\", \"is\", \"máma\"], return_missing=True)\n",
      " |          ([(2, 1)], {u'this': 1, u'is': 1})\n",
      " |  \n",
      " |  doc2idx(self, document, unknown_word_index=-1)\n",
      " |      Convert `document` (a list of words) into a list of indexes = list of `token_id`.\n",
      " |      Replace all unknown words i.e, words not in the dictionary with the index as set via `unknown_word_index`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      document : list of str\n",
      " |          Input document\n",
      " |      unknown_word_index : int, optional\n",
      " |          Index to use for words not in the dictionary.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of int\n",
      " |          Token ids for tokens in `document`, in the same order.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>>\n",
      " |          >>> corpus = [[\"a\", \"a\", \"b\"], [\"a\", \"c\"]]\n",
      " |          >>> dct = Dictionary(corpus)\n",
      " |          >>> dct.doc2idx([\"a\", \"a\", \"c\", \"not_in_dictionary\", \"c\"])\n",
      " |          [0, 0, 2, -1, 2]\n",
      " |  \n",
      " |  filter_extremes(self, no_below=5, no_above=0.5, keep_n=100000, keep_tokens=None)\n",
      " |      Filter out tokens in the dictionary by their frequency.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      no_below : int, optional\n",
      " |          Keep tokens which are contained in at least `no_below` documents.\n",
      " |      no_above : float, optional\n",
      " |          Keep tokens which are contained in no more than `no_above` documents\n",
      " |          (fraction of total corpus size, not an absolute number).\n",
      " |      keep_n : int, optional\n",
      " |          Keep only the first `keep_n` most frequent tokens.\n",
      " |      keep_tokens : iterable of str\n",
      " |          Iterable of tokens that **must** stay in dictionary after filtering.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This removes all tokens in the dictionary that are:\n",
      " |      \n",
      " |      #. Less frequent than `no_below` documents (absolute number, e.g. `5`) or \n",
      " |      \n",
      " |      #. More frequent than `no_above` documents (fraction of the total corpus size, e.g. `0.3`).\n",
      " |      #. After (1) and (2), keep only the first `keep_n` most frequent tokens (or keep all if `keep_n=None`).\n",
      " |      \n",
      " |      After the pruning, resulting gaps in word ids are shrunk.\n",
      " |      Due to this gap shrinking, **the same word may have a different word id before and after the call\n",
      " |      to this function!**\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>>\n",
      " |          >>> corpus = [[\"máma\", \"mele\", \"maso\"], [\"ema\", \"má\", \"máma\"]]\n",
      " |          >>> dct = Dictionary(corpus)\n",
      " |          >>> len(dct)\n",
      " |          5\n",
      " |          >>> dct.filter_extremes(no_below=1, no_above=0.5, keep_n=1)\n",
      " |          >>> len(dct)\n",
      " |          1\n",
      " |  \n",
      " |  filter_n_most_frequent(self, remove_n)\n",
      " |      Filter out the 'remove_n' most frequent tokens that appear in the documents.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      remove_n : int\n",
      " |          Number of the most frequent tokens that will be removed.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>>\n",
      " |          >>> corpus = [[\"máma\", \"mele\", \"maso\"], [\"ema\", \"má\", \"máma\"]]\n",
      " |          >>> dct = Dictionary(corpus)\n",
      " |          >>> len(dct)\n",
      " |          5\n",
      " |          >>> dct.filter_n_most_frequent(2)\n",
      " |          >>> len(dct)\n",
      " |          3\n",
      " |  \n",
      " |  filter_tokens(self, bad_ids=None, good_ids=None)\n",
      " |      Remove the selected `bad_ids` tokens from :class:`~gensim.corpora.dictionary.Dictionary`.\n",
      " |      \n",
      " |      Alternatively, keep selected `good_ids` in :class:`~gensim.corpora.dictionary.Dictionary` and remove the rest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      bad_ids : iterable of int, optional\n",
      " |          Collection of word ids to be removed.\n",
      " |      good_ids : collection of int, optional\n",
      " |          Keep selected collection of word ids and remove the rest.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>>\n",
      " |          >>> corpus = [[\"máma\", \"mele\", \"maso\"], [\"ema\", \"má\", \"máma\"]]\n",
      " |          >>> dct = Dictionary(corpus)\n",
      " |          >>> 'ema' in dct.token2id\n",
      " |          True\n",
      " |          >>> dct.filter_tokens(bad_ids=[dct.token2id['ema']])\n",
      " |          >>> 'ema' in dct.token2id\n",
      " |          False\n",
      " |          >>> len(dct)\n",
      " |          4\n",
      " |          >>> dct.filter_tokens(good_ids=[dct.token2id['maso']])\n",
      " |          >>> len(dct)\n",
      " |          1\n",
      " |  \n",
      " |  iteritems(self)\n",
      " |  \n",
      " |  iterkeys = __iter__(self)\n",
      " |  \n",
      " |  itervalues(self)\n",
      " |  \n",
      " |  keys(self)\n",
      " |      Get all stored ids.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of int\n",
      " |          List of all token ids.\n",
      " |  \n",
      " |  merge_with(self, other)\n",
      " |      Merge another dictionary into this dictionary, mapping the same tokens to the same ids\n",
      " |      and new tokens to new ids.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The purpose is to merge two corpora created using two different dictionaries: `self` and `other`.\n",
      " |      `other` can be any id=>word mapping (a dict, a Dictionary object, ...).\n",
      " |      \n",
      " |      Return a transformation object which, when accessed as `result[doc_from_other_corpus]`, will convert documents\n",
      " |      from a corpus built using the `other` dictionary into a document using the new, merged dictionary.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : {dict, :class:`~gensim.corpora.dictionary.Dictionary`}\n",
      " |          Other dictionary.\n",
      " |      \n",
      " |      Return\n",
      " |      ------\n",
      " |      :class:`gensim.models.VocabTransform`\n",
      " |          Transformation object.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>>\n",
      " |          >>> corpus_1, corpus_2 = [[\"a\", \"b\", \"c\"]], [[\"a\", \"f\", \"f\"]]\n",
      " |          >>> dct_1, dct_2 = Dictionary(corpus_1), Dictionary(corpus_2)\n",
      " |          >>> dct_1.doc2bow(corpus_2[0])\n",
      " |          [(0, 1)]\n",
      " |          >>> transformer = dct_1.merge_with(dct_2)\n",
      " |          >>> dct_1.doc2bow(corpus_2[0])\n",
      " |          [(0, 1), (3, 2)]\n",
      " |  \n",
      " |  patch_with_special_tokens(self, special_token_dict)\n",
      " |      Patch token2id and id2token using a dictionary of special tokens.\n",
      " |      \n",
      " |      \n",
      " |      **Usecase:** when doing sequence modeling (e.g. named entity recognition), one may  want to specify\n",
      " |      special tokens that behave differently than others.\n",
      " |      One example is the \"unknown\" token, and another is the padding token.\n",
      " |      It is usual to set the padding token to have index `0`, and patching the dictionary with `{'<PAD>': 0}`\n",
      " |      would be one way to specify this.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      special_token_dict : dict of (str, int)\n",
      " |          dict containing the special tokens as keys and their wanted indices as values.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>>\n",
      " |          >>> corpus = [[\"máma\", \"mele\", \"maso\"], [\"ema\", \"má\", \"máma\"]]\n",
      " |          >>> dct = Dictionary(corpus)\n",
      " |          >>>\n",
      " |          >>> special_tokens = {'pad': 0, 'space': 1}\n",
      " |          >>> print(dct.token2id)\n",
      " |          {'maso': 0, 'mele': 1, 'máma': 2, 'ema': 3, 'má': 4}\n",
      " |          >>>\n",
      " |          >>> dct.patch_with_special_tokens(special_tokens)\n",
      " |          >>> print(dct.token2id)\n",
      " |          {'maso': 6, 'mele': 7, 'máma': 2, 'ema': 3, 'má': 4, 'pad': 0, 'space': 1}\n",
      " |  \n",
      " |  save_as_text(self, fname, sort_by_word=True)\n",
      " |      Save :class:`~gensim.corpora.dictionary.Dictionary` to a text file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to output file.\n",
      " |      sort_by_word : bool, optional\n",
      " |          Sort words in lexicographical order before writing them out?\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Format::\n",
      " |      \n",
      " |          num_docs\n",
      " |          id_1[TAB]word_1[TAB]document_frequency_1[NEWLINE]\n",
      " |          id_2[TAB]word_2[TAB]document_frequency_2[NEWLINE]\n",
      " |          ....\n",
      " |          id_k[TAB]word_k[TAB]document_frequency_k[NEWLINE]\n",
      " |      \n",
      " |      This text format is great for corpus inspection and debugging. As plaintext, it's also easily portable\n",
      " |      to other tools and frameworks. For better performance and to store the entire object state,\n",
      " |      including collected corpus statistics, use :meth:`~gensim.corpora.dictionary.Dictionary.save` and\n",
      " |      :meth:`~gensim.corpora.dictionary.Dictionary.load` instead.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.corpora.dictionary.Dictionary.load_from_text`\n",
      " |          Load :class:`~gensim.corpora.dictionary.Dictionary` from text file.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>> from gensim.test.utils import get_tmpfile\n",
      " |          >>>\n",
      " |          >>> tmp_fname = get_tmpfile(\"dictionary\")\n",
      " |          >>> corpus = [[\"máma\", \"mele\", \"maso\"], [\"ema\", \"má\", \"máma\"]]\n",
      " |          >>>\n",
      " |          >>> dct = Dictionary(corpus)\n",
      " |          >>> dct.save_as_text(tmp_fname)\n",
      " |          >>>\n",
      " |          >>> loaded_dct = Dictionary.load_from_text(tmp_fname)\n",
      " |          >>> assert dct.token2id == loaded_dct.token2id\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  from_corpus(corpus, id2word=None)\n",
      " |      Create :class:`~gensim.corpora.dictionary.Dictionary` from an existing corpus.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus : iterable of iterable of (int, number)\n",
      " |          Corpus in BoW format.\n",
      " |      id2word : dict of (int, object)\n",
      " |          Mapping id -> word. If None, the mapping `id2word[word_id] = str(word_id)` will be used.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This can be useful if you only have a term-document BOW matrix (represented by `corpus`), but not the original\n",
      " |      text corpus. This method will scan the term-document count matrix for all word ids that appear in it,\n",
      " |      then construct :class:`~gensim.corpora.dictionary.Dictionary` which maps each `word_id -> id2word[word_id]`.\n",
      " |      `id2word` is an optional dictionary that maps the `word_id` to a token.\n",
      " |      In case `id2word` isn't specified the mapping `id2word[word_id] = str(word_id)` will be used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.corpora.dictionary.Dictionary`\n",
      " |          Inferred dictionary from corpus.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>>\n",
      " |          >>> corpus = [[(1, 1.0)], [], [(0, 5.0), (2, 1.0)], []]\n",
      " |          >>> dct = Dictionary.from_corpus(corpus)\n",
      " |          >>> len(dct)\n",
      " |          3\n",
      " |  \n",
      " |  from_documents(documents)\n",
      " |      Create :class:`~gensim.corpora.dictionary.Dictionary` from `documents`.\n",
      " |      \n",
      " |      Equivalent to `Dictionary(documents=documents)`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      documents : iterable of iterable of str\n",
      " |          Input corpus.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.corpora.dictionary.Dictionary`\n",
      " |          Dictionary initialized from `documents`.\n",
      " |  \n",
      " |  load_from_text(fname)\n",
      " |      Load a previously stored :class:`~gensim.corpora.dictionary.Dictionary` from a text file.\n",
      " |      \n",
      " |      Mirror function to :meth:`~gensim.corpora.dictionary.Dictionary.save_as_text`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname: str\n",
      " |          Path to a file produced by :meth:`~gensim.corpora.dictionary.Dictionary.save_as_text`.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.corpora.dictionary.Dictionary.save_as_text`\n",
      " |          Save :class:`~gensim.corpora.dictionary.Dictionary` to text file.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>> from gensim.test.utils import get_tmpfile\n",
      " |          >>>\n",
      " |          >>> tmp_fname = get_tmpfile(\"dictionary\")\n",
      " |          >>> corpus = [[\"máma\", \"mele\", \"maso\"], [\"ema\", \"má\", \"máma\"]]\n",
      " |          >>>\n",
      " |          >>> dct = Dictionary(corpus)\n",
      " |          >>> dct.save_as_text(tmp_fname)\n",
      " |          >>>\n",
      " |          >>> loaded_dct = Dictionary.load_from_text(tmp_fname)\n",
      " |          >>> assert dct.token2id == loaded_dct.token2id\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset(), pickle_protocol=2)\n",
      " |      Save the object to a file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname_or_handle : str or file-like\n",
      " |          Path to output file or already opened file-like object. If the object is a file handle,\n",
      " |          no special array handling will be performed, all attributes will be saved to the same file.\n",
      " |      separately : list of str or None, optional\n",
      " |          If None, automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      " |          them into separate files. This prevent memory errors for large objects, and also allows\n",
      " |          `memory-mapping <https://en.wikipedia.org/wiki/Mmap>`_ the large arrays for efficient\n",
      " |          loading and sharing the large arrays in RAM between multiple processes.\n",
      " |      \n",
      " |          If list of str: store these attributes into separate files. The automated size check\n",
      " |          is not performed in this case.\n",
      " |      sep_limit : int, optional\n",
      " |          Don't store arrays smaller than this separately. In bytes.\n",
      " |      ignore : frozenset of str, optional\n",
      " |          Attributes that shouldn't be stored at all.\n",
      " |      pickle_protocol : int, optional\n",
      " |          Protocol number for pickle.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.utils.SaveLoad.load`\n",
      " |          Load object from file.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  load(fname, mmap=None) from abc.ABCMeta\n",
      " |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to file that contains needed object.\n",
      " |      mmap : str, optional\n",
      " |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      " |          via mmap (shared memory) using `mmap='r'.\n",
      " |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.utils.SaveLoad.save`\n",
      " |          Save object to file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      object\n",
      " |          Object loaded from `fname`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AttributeError\n",
      " |          When called on an object instance instead of class (this is a class method).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from collections.abc.Mapping:\n",
      " |  \n",
      " |  __contains__(self, key)\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  get(self, key, default=None)\n",
      " |      D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n",
      " |  \n",
      " |  items(self)\n",
      " |      D.items() -> a set-like object providing a view on D's items\n",
      " |  \n",
      " |  values(self)\n",
      " |      D.values() -> an object providing a view on D's values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from collections.abc.Mapping:\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __reversed__ = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from collections.abc.Collection:\n",
      " |  \n",
      " |  __subclasshook__(C) from abc.ABCMeta\n",
      " |      Abstract classes can override this to customize issubclass().\n",
      " |      \n",
      " |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      " |      It should return True, False or NotImplemented.  If it returns\n",
      " |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      " |      overrides the normal algorithm (and the outcome is cached).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У объекта dictionary есть полезная переменная dictionary.token2id, позволяющая находить соответствие между ингредиентами и их индексами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели\n",
    "Вам может понадобиться [документация](https://radimrehurek.com/gensim/models/ldamodel.html) LDA в gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 1.__ Обучите модель LDA с 40 темами, установив количество проходов по коллекции 5 и оставив остальные параметры по умолчанию. \n",
    "\n",
    "\n",
    "Затем вызовите метод модели *show_topics*, указав количество тем 40 и количество токенов 10, и сохраните результат (топы ингредиентов в темах) в отдельную переменную. Если при вызове метода *show_topics* указать параметр *formatted=True*, то топы ингредиентов будет удобно выводить на печать, если *formatted=False*, будет удобно работать со списком программно. Выведите топы на печать, рассмотрите темы, а затем ответьте на вопрос:\n",
    "\n",
    "Сколько раз ингредиенты \"salt\", \"sugar\", \"water\", \"mushrooms\", \"chicken\", \"eggs\" встретились среди топов-10 всех 40 тем? При ответе __не нужно__ учитывать составные ингредиенты, например, \"hot water\".\n",
    "\n",
    "Передайте 6 чисел в функцию save_answers1 и загрузите сгенерированный файл в форму.\n",
    "\n",
    "У gensim нет возможности фиксировать случайное приближение через параметры метода, но библиотека использует numpy для инициализации матриц. Поэтому, по утверждению автора библиотеки, фиксировать случайное приближение нужно командой, которая написана в следующей ячейке. __Перед строкой кода с построением модели обязательно вставляйте указанную строку фиксации random.seed.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(76543)\n",
    "# здесь код для построения модели:\n",
    "lda = LdaModel(corpus, num_topics=40, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "resul=lda.show_topics(num_topics=40, num_words=10, formatted=False)\n",
    "#print(resul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_salt=0\n",
    "c_sugar=0\n",
    "c_water=0\n",
    "c_mushrooms=0\n",
    "c_chicken=0\n",
    "c_eggs=0\n",
    "#\"salt\", \"sugar\", \"water\", \"mushrooms\", \"chicken\", \"eggs\"\n",
    "for i in range(40):\n",
    "    for j in range(10):\n",
    "        if dict2[int(resul[i][1][j][0])]==\"salt\":\n",
    "            c_salt+=1\n",
    "        if dict2[int(resul[i][1][j][0])]==\"sugar\":\n",
    "            c_sugar+=1\n",
    "        if dict2[int(resul[i][1][j][0])]==\"water\":\n",
    "            c_water+=1\n",
    "        if dict2[int(resul[i][1][j][0])]==\"mushrooms\":\n",
    "            c_mushrooms+=1\n",
    "        if dict2[int(resul[i][1][j][0])]==\"chicken\":\n",
    "            c_chicken+=1\n",
    "        if dict2[int(resul[i][1][j][0])]==\"eggs\":\n",
    "            c_eggs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_salt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(resul[0][1][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers1(c_salt, c_sugar, c_water, c_mushrooms, c_chicken, c_eggs):\n",
    "    with open(\"cooking_LDA_pa_task1.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(el) for el in [c_salt, c_sugar, c_water, c_mushrooms, c_chicken, c_eggs]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_answers1(c_salt, c_sugar, c_water, c_mushrooms, c_chicken, c_eggs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фильтрация словаря\n",
    "В топах тем гораздо чаще встречаются первые три рассмотренных ингредиента, чем последние три. При этом наличие в рецепте курицы, яиц и грибов яснее дает понять, что мы будем готовить, чем наличие соли, сахара и воды. Таким образом, даже в рецептах есть слова, часто встречающиеся в текстах и не несущие смысловой нагрузки, и поэтому их не желательно видеть в темах. Наиболее простой прием борьбы с такими фоновыми элементами — фильтрация словаря по частоте. Обычно словарь фильтруют с двух сторон: убирают очень редкие слова (в целях экономии памяти) и очень частые слова (в целях повышения интерпретируемости тем). Мы уберем только частые слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "dictionary2 = copy.deepcopy(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 2.__ У объекта dictionary2 есть переменная *dfs* — это словарь, ключами которого являются id токена, а элементами — число раз, сколько слово встретилось во всей коллекции. Сохраните в отдельный список ингредиенты, которые встретились в коллекции больше 4000 раз. Вызовите метод словаря *filter_tokens*, подав в качестве первого аргумента полученный список популярных ингредиентов. Вычислите две величины: dict_size_before и dict_size_after — размер словаря до и после фильтрации.\n",
    "\n",
    "Затем, используя новый словарь, создайте новый корпус документов, corpus2, по аналогии с тем, как это сделано в начале ноутбука. Вычислите две величины: corpus_size_before и corpus_size_after — суммарное количество ингредиентов в корпусе (для каждого документа вычислите число различных ингредиентов в нем и просуммируйте по всем документам) до и после фильтрации.\n",
    "\n",
    "Передайте величины dict_size_before, dict_size_after, corpus_size_before, corpus_size_after в функцию save_answers2 и загрузите сгенерированный файл в форму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_dfs0=dictionary2.dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_dfs={}\n",
    "for key in dic_dfs0:\n",
    "    if dic_dfs0[key]>4000:\n",
    "        dic_dfs[key]=dic_dfs0[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 7380,\n",
       " 5: 4438,\n",
       " 15: 18048,\n",
       " 11: 4784,\n",
       " 18: 4385,\n",
       " 20: 4847,\n",
       " 29: 7457,\n",
       " 44: 7972,\n",
       " 52: 6434,\n",
       " 59: 7971,\n",
       " 104: 6236,\n",
       " 114: 4632}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6714\n"
     ]
    }
   ],
   "source": [
    "dict_size_before=len(dictionary)\n",
    "print(dict_size_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary2.filter_tokens(dic_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6702\n"
     ]
    }
   ],
   "source": [
    "dict_size_after=len(dictionary2)\n",
    "print(dict_size_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus2 = [dictionary2.doc2bow(text) for text in texts]  # составляем корпус документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39774"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39774"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_corp_1=0\n",
    "for i in range(len(corpus)):\n",
    "    sum_corp_1+=len(corpus[i])\n",
    "sum_corp_2=0\n",
    "for i in range(len(corpus2)):\n",
    "    sum_corp_2+=len(corpus2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "428249"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_corp_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343665"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_corp_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers2(dict_size_before, dict_size_after, corpus_size_before, corpus_size_after):\n",
    "    with open(\"cooking_LDA_pa_task2.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(el) for el in [dict_size_before, dict_size_after, corpus_size_before, corpus_size_after]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_answers2(dict_size_before, dict_size_after, sum_corp_1, sum_corp_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение когерентностей\n",
    "__Задание 3.__ Постройте еще одну модель по корпусу corpus2 и словарю dictionary2, остальные параметры оставьте такими же, как при первом построении модели. Сохраните новую модель в другую переменную (не перезаписывайте предыдущую модель). Не забудьте про фиксирование seed!\n",
    "\n",
    "Затем воспользуйтесь методом *top_topics* модели, чтобы вычислить ее когерентность. Передайте в качестве аргумента соответствующий модели корпус. Метод вернет список кортежей (топ токенов, когерентность), отсортированных по убыванию последней. Вычислите среднюю по всем темам когерентность для каждой из двух моделей и передайте в функцию save_answers3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(76543)\n",
    "# здесь код для построения модели:\n",
    "lda2 = LdaModel(corpus2, num_topics=40, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dictionary' object has no attribute 'id2word'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-143-3b16943e7621>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlda2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtop_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictionary2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Dictionary' object has no attribute 'id2word'"
     ]
    }
   ],
   "source": [
    "lda2.top_topics(dictionary2.id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method top_topics in module gensim.models.ldamodel:\n",
      "\n",
      "top_topics(corpus=None, texts=None, dictionary=None, window_size=None, coherence='u_mass', topn=20, processes=-1) method of gensim.models.ldamodel.LdaModel instance\n",
      "    Get the topics with the highest coherence score the coherence for each topic.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    corpus : iterable of list of (int, float), optional\n",
      "        Corpus in BoW format.\n",
      "    texts : list of list of str, optional\n",
      "        Tokenized texts, needed for coherence models that use sliding window based (i.e. coherence=`c_something`)\n",
      "        probability estimator .\n",
      "    dictionary : :class:`~gensim.corpora.dictionary.Dictionary`, optional\n",
      "        Gensim dictionary mapping of id word to create corpus.\n",
      "        If `model.id2word` is present, this is not needed. If both are provided, passed `dictionary` will be used.\n",
      "    window_size : int, optional\n",
      "        Is the size of the window to be used for coherence measures using boolean sliding window as their\n",
      "        probability estimator. For 'u_mass' this doesn't matter.\n",
      "        If None - the default window sizes are used which are: 'c_v' - 110, 'c_uci' - 10, 'c_npmi' - 10.\n",
      "    coherence : {'u_mass', 'c_v', 'c_uci', 'c_npmi'}, optional\n",
      "        Coherence measure to be used.\n",
      "        Fastest method - 'u_mass', 'c_uci' also known as `c_pmi`.\n",
      "        For 'u_mass' corpus should be provided, if texts is provided, it will be converted to corpus\n",
      "        using the dictionary. For 'c_v', 'c_uci' and 'c_npmi' `texts` should be provided (`corpus` isn't needed)\n",
      "    topn : int, optional\n",
      "        Integer corresponding to the number of top words to be extracted from each topic.\n",
      "    processes : int, optional\n",
      "        Number of processes to use for probability estimation phase, any value less than 1 will be interpreted as\n",
      "        num_cpus - 1.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    list of (list of (int, str), float)\n",
      "        Each element in the list is a pair of a topic representation and its coherence score. Topic representations\n",
      "        are distributions of words, represented as a list of pairs of word IDs and their probabilities.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lda2.top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers3(coherence, coherence2):\n",
    "    with open(\"cooking_LDA_pa_task3.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([\"%3f\"%el for el in [coherence, coherence2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считается, что когерентность хорошо соотносится с человеческими оценками интерпретируемости тем. Поэтому на больших текстовых коллекциях когерентность обычно повышается, если убрать фоновую лексику. Однако в нашем случае этого не произошло. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изучение влияния гиперпараметра alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом разделе мы будем работать со второй моделью, то есть той, которая построена по сокращенному корпусу. \n",
    "\n",
    "Пока что мы посмотрели только на матрицу темы-слова, теперь давайте посмотрим на матрицу темы-документы. Выведите темы для нулевого (или любого другого) документа из корпуса, воспользовавшись методом *get_document_topics* второй модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(25, 0.12812185), (31, 0.6175929), (33, 0.13865705)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda2.get_document_topics(corpus2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также выведите содержимое переменной *.alpha* второй модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n",
       "       0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n",
       "       0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n",
       "       0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n",
       "       0.025, 0.025, 0.025, 0.025], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda2.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У вас должно получиться, что документ характеризуется небольшим числом тем. Попробуем поменять гиперпараметр alpha, задающий априорное распределение Дирихле для распределений тем в документах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 4.__ Обучите третью модель: используйте сокращенный корпус (corpus2 и dictionary2) и установите параметр __alpha=1__, passes=5. Не забудьте про фиксацию seed! Выведите темы новой модели для нулевого документа; должно получиться, что распределение над множеством тем практически равномерное. Чтобы убедиться в том, что во второй модели документы описываются гораздо более разреженными распределениями, чем в третьей, посчитайте суммарное количество элементов, __превосходящих 0.01__, в матрицах темы-документы обеих моделей. Другими словами, запросите темы  модели для каждого документа с параметром *minimum_probability=0.01* и просуммируйте число элементов в получаемых массивах. Передайте две суммы (сначала для модели с alpha по умолчанию, затем для модели в alpha=1) в функцию save_answers4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(76543)\n",
    "lda3 = LdaModel(corpus2, num_topics=40,  alpha=1, passes=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(76543)\n",
    "lda4 = LdaModel(corpus2, num_topics=40,  passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda3.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(lda3.get_document_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39774"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39774"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "count2=0\n",
    "count3=0\n",
    "for i in range(len(corpus2)):\n",
    "    count2+=len(lda2.get_document_topics(corpus2[i],minimum_probability=0.01))\n",
    "    count3+=len(lda3.get_document_topics(corpus2[i],minimum_probability=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203699"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1590960"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.022051143),\n",
       " (1, 0.021793237),\n",
       " (2, 0.02099149),\n",
       " (3, 0.035287727),\n",
       " (4, 0.022173652),\n",
       " (5, 0.059873417),\n",
       " (6, 0.021001473),\n",
       " (7, 0.021479024),\n",
       " (8, 0.021507202),\n",
       " (9, 0.02165233),\n",
       " (10, 0.021822779),\n",
       " (11, 0.023851201),\n",
       " (12, 0.020867417),\n",
       " (13, 0.02420217),\n",
       " (14, 0.021085134),\n",
       " (15, 0.021173252),\n",
       " (16, 0.04360766),\n",
       " (17, 0.021456068),\n",
       " (18, 0.020908482),\n",
       " (19, 0.021586815),\n",
       " (20, 0.023564022),\n",
       " (21, 0.020853054),\n",
       " (22, 0.020839604),\n",
       " (23, 0.020845221),\n",
       " (24, 0.020922503),\n",
       " (25, 0.021112766),\n",
       " (26, 0.02193436),\n",
       " (27, 0.042701177),\n",
       " (28, 0.025113722),\n",
       " (29, 0.04209216),\n",
       " (30, 0.020846361),\n",
       " (31, 0.021597417),\n",
       " (32, 0.021373933),\n",
       " (33, 0.020853352),\n",
       " (34, 0.022098338),\n",
       " (35, 0.021026993),\n",
       " (36, 0.03729422),\n",
       " (37, 0.021366818),\n",
       " (38, 0.020833688),\n",
       " (39, 0.024358619)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda3.get_document_topics(corpus2[1],minimum_probability=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method show_topics in module gensim.models.ldamodel:\n",
      "\n",
      "show_topics(num_topics=10, num_words=10, log=False, formatted=True) method of gensim.models.ldamodel.LdaModel instance\n",
      "    Get a representation for selected topics.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    num_topics : int, optional\n",
      "        Number of topics to be returned. Unlike LSA, there is no natural ordering between the topics in LDA.\n",
      "        The returned topics subset of all topics is therefore arbitrary and may change between two LDA\n",
      "        training runs.\n",
      "    num_words : int, optional\n",
      "        Number of words to be presented for each topic. These will be the most relevant words (assigned the highest\n",
      "        probability for each topic).\n",
      "    log : bool, optional\n",
      "        Whether the output is also logged, besides being returned.\n",
      "    formatted : bool, optional\n",
      "        Whether the topic representations should be formatted as strings. If False, they are returned as\n",
      "        2 tuples of (word, probability).\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    list of {str, tuple of (str, float)}\n",
      "        a list of topics, each represented either as a string (when `formatted` == True) or word-probability\n",
      "        pairs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lda.show_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers4(count_model2, count_model3):\n",
    "    with open(\"cooking_LDA_pa_task4.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(el) for el in [count_model2, count_model3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, гиперпараметр __alpha__ влияет на разреженность распределений тем в документах. Аналогично гиперпараметр __eta__ влияет на разреженность распределений слов в темах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_answers4(count2, count3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA как способ понижения размерности\n",
    "Иногда, распределения над темами, найденные с помощью LDA, добавляют в матрицу объекты-признаки как дополнительные, семантические, признаки, и это может улучшить качество решения задачи. Для простоты давайте просто обучим классификатор рецептов на кухни на признаках, полученных из LDA, и измерим точность (accuracy).\n",
    "\n",
    "__Задание 5.__ Используйте модель, построенную по сокращенной выборке с alpha по умолчанию (вторую модель). Составьте матрицу $\\Theta = p(t|d)$ вероятностей тем в документах; вы можете использовать тот же метод get_document_topics, а также вектор правильных ответов y (в том же порядке, в котором рецепты идут в переменной recipes). Создайте объект RandomForestClassifier со 100 деревьями, с помощью функции cross_val_score вычислите среднюю accuracy по трем фолдам (перемешивать данные не нужно) и передайте в функцию save_answers5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers5(accuracy):\n",
    "     with open(\"cooking_LDA_pa_task5.txt\", \"w\") as fout:\n",
    "        fout.write(str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для такого большого количества классов это неплохая точность. Вы можете попроовать обучать RandomForest на исходной матрице частот слов, имеющей значительно большую размерность, и увидеть, что accuracy увеличивается на 10–15%. Таким образом, LDA собрал не всю, но достаточно большую часть информации из выборки, в матрице низкого ранга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA — вероятностная модель\n",
    "Матричное разложение, использующееся в LDA, интерпретируется как следующий процесс генерации документов.\n",
    "\n",
    "Для документа $d$ длины $n_d$:\n",
    "1. Из априорного распределения Дирихле с параметром alpha сгенерировать распределение над множеством тем: $\\theta_d \\sim Dirichlet(\\alpha)$\n",
    "1. Для каждого слова $w = 1, \\dots, n_d$:\n",
    "    1. Сгенерировать тему из дискретного распределения $t \\sim \\theta_{d}$\n",
    "    1. Сгенерировать слово из дискретного распределения $w \\sim \\phi_{t}$.\n",
    "    \n",
    "Подробнее об этом в [Википедии](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation).\n",
    "\n",
    "В контексте нашей задачи получается, что, используя данный генеративный процесс, можно создавать новые рецепты. Вы можете передать в функцию модель и число ингредиентов и сгенерировать рецепт :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recipe(model, num_ingredients):\n",
    "    theta = np.random.dirichlet(model.alpha)\n",
    "    for i in range(num_ingredients):\n",
    "        t = np.random.choice(np.arange(model.num_topics), p=theta)\n",
    "        topic = model.show_topic(t, topn=model.num_terms)\n",
    "        topic_distr = [x[1] for x in topic]\n",
    "        terms = [x[0] for x in topic]\n",
    "        w = np.random.choice(terms, p=topic_distr)\n",
    "        print w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Интерпретация построенной модели\n",
    "Вы можете рассмотреть топы ингредиентов каждой темы. Большиснтво тем сами по себе похожи на рецепты; в некоторых собираются продукты одного вида, например, свежие фрукты или разные виды сыра.\n",
    "\n",
    "Попробуем эмпирически соотнести наши темы с национальными кухнями (cuisine). Построим матрицу $A$ размера темы $x$ кухни, ее элементы $a_{tc}$ — суммы $p(t|d)$ по всем документам $d$, которые отнесены к кухне $c$. Нормируем матрицу на частоты рецептов по разным кухням, чтобы избежать дисбаланса между кухнями. Следующая функция получает на вход объект модели, объект корпуса и исходные данные и возвращает нормированную матрицу $A$. Ее удобно визуализировать с помощью seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import seaborn\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_topic_cuisine_matrix(model, corpus, recipes):\n",
    "    # составляем вектор целевых признаков\n",
    "    targets = list(set([recipe[\"cuisine\"] for recipe in recipes]))\n",
    "    # составляем матрицу\n",
    "    tc_matrix = pandas.DataFrame(data=np.zeros((model.num_topics, len(targets))), columns=targets)\n",
    "    for recipe, bow in zip(recipes, corpus):\n",
    "        recipe_topic = model.get_document_topics(bow)\n",
    "        for t, prob in recipe_topic:\n",
    "            tc_matrix[recipe[\"cuisine\"]][t] += prob\n",
    "    # нормируем матрицу\n",
    "    target_sums = pandas.DataFrame(data=np.zeros((1, len(targets))), columns=targets)\n",
    "    for recipe in recipes:\n",
    "        target_sums[recipe[\"cuisine\"]] += 1\n",
    "    return pandas.DataFrame(tc_matrix.values/target_sums.values, columns=tc_matrix.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matrix(tc_matrix):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    seaborn.heatmap(tc_matrix, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализируйте матрицу\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем темнее квадрат в матрице, тем больше связь этой темы с данной кухней. Мы видим, что у нас есть темы, которые связаны с несколькими кухнями. Такие темы показывают набор ингредиентов, которые популярны в кухнях нескольких народов, то есть указывают на схожесть кухонь этих народов. Некоторые темы распределены по всем кухням равномерно, они показывают наборы продуктов, которые часто используются в кулинарии всех стран. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Жаль, что в датасете нет названий рецептов, иначе темы было бы проще интерпретировать..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заключение\n",
    "В этом задании вы построили несколько моделей LDA, посмотрели, на что влияют гиперпараметры модели и как можно использовать построенную модель. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
